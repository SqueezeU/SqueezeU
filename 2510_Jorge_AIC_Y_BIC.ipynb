{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SqueezeU/SqueezeU/blob/main/2510_Jorge_AIC_Y_BIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claro, ¡vamos allá!\n",
        "\n",
        "En estadística y aprendizaje automático, AIC y BIC son dos criterios utilizados para evaluar modelos, especialmente en el contexto de modelos de regresión y selección de modelos. Estos criterios ayudan a determinar qué modelo tiene un mejor ajuste y complejidad, penalizando aquellos con demasiados parámetros (para evitar sobreajuste). A partir de esto, se pueden utilizar técnicas de selección de variables como *forward selection*, *backward elimination* y *stepwise selection* para escoger las variables más relevantes. A continuación, te explico cada uno de estos conceptos.\n",
        "\n",
        "### 1. Criterios de Información de Akaike (AIC) y Bayesiano (BIC)\n",
        "\n",
        "Ambos criterios son medidas de ajuste de modelos que incluyen una penalización por la cantidad de parámetros. Son especialmente útiles cuando estamos probando modelos de distinta complejidad y queremos comparar cuál se ajusta mejor a los datos.\n",
        "\n",
        "#### **AIC (Akaike Information Criterion)**\n",
        "\n",
        "- **Fórmula:**\n",
        "  \\[\n",
        "  AIC = 2k - 2 \\ln(L)\n",
        "  \\]\n",
        "  donde:\n",
        "  - \\( k \\) es el número de parámetros en el modelo,\n",
        "  - \\( L \\) es la verosimilitud del modelo dado el conjunto de datos.\n",
        "\n",
        "- **Interpretación:** El AIC busca el modelo que mejor se ajusta a los datos sin sobreajustar. Penaliza ligeramente más el ajuste que la complejidad. Entre dos modelos, el que tenga menor AIC es preferido.\n",
        "\n",
        "#### **BIC (Bayesian Information Criterion)**\n",
        "\n",
        "- **Fórmula:**\n",
        "  \\[\n",
        "  BIC = k \\ln(n) - 2 \\ln(L)\n",
        "  \\]\n",
        "  donde:\n",
        "  - \\( n \\) es el tamaño de la muestra.\n",
        "\n",
        "- **Interpretación:** El BIC penaliza la complejidad de forma más fuerte que el AIC, especialmente en muestras grandes. Este criterio tiende a seleccionar modelos más simples que el AIC, porque la penalización aumenta con el tamaño de los datos.\n",
        "\n",
        "Ambos criterios ayudan a encontrar un equilibrio entre ajuste y simplicidad del modelo, y usualmente se usan como guías, no como reglas estrictas. Un modelo con un AIC o BIC más bajo es mejor, pero no siempre el más bajo es el más interpretativo o útil.\n",
        "\n",
        "### 2. Métodos de Selección de Variables: Forward, Backward y Stepwise\n",
        "\n",
        "Estos métodos buscan identificar el subconjunto óptimo de variables para incluir en un modelo. El objetivo es maximizar el ajuste sin sobreajustar, idealmente seleccionando un subconjunto pequeño de variables significativas.\n",
        "\n",
        "#### **Método de Selección Hacia Adelante (Forward Selection)**\n",
        "\n",
        "- Empieza con un modelo sin variables.\n",
        "- Agrega iterativamente la variable que más mejora el criterio de ajuste (AIC o BIC), usualmente de mayor a menor mejora en el ajuste.\n",
        "- Detiene la inclusión de variables cuando ya no hay una mejora significativa en el criterio.\n",
        "\n",
        "Este método es útil cuando partimos de un conjunto amplio de variables y queremos identificar cuáles son las más relevantes para el modelo.\n",
        "\n",
        "#### **Método de Eliminación Hacia Atrás (Backward Elimination)**\n",
        "\n",
        "- Comienza con un modelo que incluye todas las variables.\n",
        "- En cada paso, elimina la variable que menos contribuye al ajuste del modelo (la que menos mejora el AIC o BIC).\n",
        "- Continua eliminando variables hasta que el modelo no pueda mejorar significativamente.\n",
        "\n",
        "Este método es útil cuando partimos de un conjunto pequeño o moderado de variables y queremos simplificar el modelo.\n",
        "\n",
        "#### **Método de Selección Paso a Paso (Stepwise Selection)**\n",
        "\n",
        "- Es una combinación de los métodos hacia adelante y hacia atrás.\n",
        "- En cada paso, permite tanto agregar como eliminar variables, dependiendo de si mejoran o empeoran el ajuste (basado en AIC o BIC).\n",
        "- Esto permite más flexibilidad y, a menudo, da un mejor modelo final, ya que se consideran ambas direcciones de inclusión y exclusión.\n",
        "\n",
        "### Resumen\n",
        "\n",
        "| Método             | Inicia con | Procedimiento |\n",
        "|--------------------|------------|---------------|\n",
        "| **Forward**        | Modelo vacío | Agrega variables una por una, basado en mejora del ajuste |\n",
        "| **Backward**       | Modelo completo | Elimina variables una por una, basándose en el ajuste |\n",
        "| **Stepwise**       | Puede iniciar vacío o completo | Alterna entre agregar y quitar variables según el ajuste |\n",
        "\n",
        "### Aplicación Práctica\n",
        "\n",
        "1. **Elegir el criterio**: AIC o BIC, dependiendo de si prefieres un modelo más complejo (AIC) o uno más sencillo (BIC).\n",
        "2. **Aplicar el método**: Según el número de variables y el enfoque (agregar o quitar), elige el método de selección.\n",
        "3. **Evaluar el resultado**: Observa el AIC/BIC final y asegúrate de que el modelo se ajuste bien a los datos y sea interpretativamente útil.\n",
        "\n",
        "Estos métodos son muy utilizados en modelado estadístico y aprendizaje automático, ya que permiten construir modelos eficientes sin sobreajustar, balanceando ajuste y simplicidad."
      ],
      "metadata": {
        "id": "1LrFL-AKb9Kj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d17bXXSub8JZ",
        "outputId": "5422a85a-6972-4294-8669-081c99c99878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables seleccionadas por Forward Selection: Index(['MedInc', 'HouseAge', 'AveBedrms', 'Latitude', 'Longitude'], dtype='object')\n",
            "AIC Forward: 36439.22847622834\n",
            "BIC Forward: 36485.49953223803\n",
            "Variables seleccionadas por Backward Elimination: Index(['MedInc', 'HouseAge', 'AveBedrms', 'Latitude', 'Longitude'], dtype='object')\n",
            "AIC Backward: 36439.22847622834\n",
            "BIC Backward: 36485.49953223803\n",
            "Variables seleccionadas por Stepwise Selection: ['MedInc', 'HouseAge', 'Latitude', 'Longitude', 'AveBedrms', 'AveRooms', 'AveOccup']\n",
            "AIC Stepwise: 36188.37407476429\n",
            "BIC Stepwise: 36250.068816110535\n",
            "\n",
            "Comparación de Modelos:\n",
            "Forward Selection - AIC: 36439.22847622834, BIC: 36485.49953223803\n",
            "Backward Elimination - AIC: 36439.22847622834, BIC: 36485.49953223803\n",
            "Stepwise Selection - AIC: 36188.37407476429, BIC: 36250.068816110535\n"
          ]
        }
      ],
      "source": [
        "# Importar librerías necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE, SequentialFeatureSelector\n",
        "from statsmodels.tools import add_constant\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Cargar el conjunto de datos California Housing\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Función para calcular AIC y BIC\n",
        "def calculate_aic_bic(model, X, y):\n",
        "    X_sm = add_constant(X)  # Agregar constante para el intercepto\n",
        "    model_sm = sm.OLS(y, X_sm).fit()\n",
        "    return model_sm.aic, model_sm.bic\n",
        "\n",
        "# 1. Método de Selección Hacia Adelante (Forward Selection)\n",
        "forward_selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction='forward')\n",
        "forward_selector.fit(X_train, y_train)\n",
        "selected_forward = X_train.columns[forward_selector.get_support()]\n",
        "print(\"Variables seleccionadas por Forward Selection:\", selected_forward)\n",
        "\n",
        "# Calcular AIC y BIC para el modelo Forward\n",
        "aic_forward, bic_forward = calculate_aic_bic(LinearRegression(), X_train[selected_forward], y_train)\n",
        "print(\"AIC Forward:\", aic_forward)\n",
        "print(\"BIC Forward:\", bic_forward)\n",
        "\n",
        "# 2. Método de Eliminación Hacia Atrás (Backward Elimination)\n",
        "backward_selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction='backward')\n",
        "backward_selector.fit(X_train, y_train)\n",
        "selected_backward = X_train.columns[backward_selector.get_support()]\n",
        "print(\"Variables seleccionadas por Backward Elimination:\", selected_backward)\n",
        "\n",
        "# Calcular AIC y BIC para el modelo Backward\n",
        "aic_backward, bic_backward = calculate_aic_bic(LinearRegression(), X_train[selected_backward], y_train)\n",
        "print(\"AIC Backward:\", aic_backward)\n",
        "print(\"BIC Backward:\", bic_backward)\n",
        "\n",
        "# 3. Método de Selección Paso a Paso (Stepwise Selection)\n",
        "def stepwise_selection(X, y, initial_list=[], threshold_in=0.01, threshold_out=0.05):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "        excluded = list(set(X.columns) - set(included))\n",
        "        new_pval = pd.Series(index=excluded)\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(X[included + [new_column]])).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        best_pval = new_pval.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_feature = new_pval.idxmin()\n",
        "            included.append(best_feature)\n",
        "            changed = True\n",
        "\n",
        "        model = sm.OLS(y, sm.add_constant(X[included])).fit()\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max()\n",
        "        if worst_pval > threshold_out:\n",
        "            changed = True\n",
        "            worst_feature = pvalues.idxmax()\n",
        "            included.remove(worst_feature)\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return included\n",
        "\n",
        "# Ejecutar Stepwise selection\n",
        "selected_stepwise = stepwise_selection(X_train, y_train)\n",
        "print(\"Variables seleccionadas por Stepwise Selection:\", selected_stepwise)\n",
        "\n",
        "# Calcular AIC y BIC para el modelo Stepwise\n",
        "aic_stepwise, bic_stepwise = calculate_aic_bic(LinearRegression(), X_train[selected_stepwise], y_train)\n",
        "print(\"AIC Stepwise:\", aic_stepwise)\n",
        "print(\"BIC Stepwise:\", bic_stepwise)\n",
        "\n",
        "# Comparación de Modelos\n",
        "print(\"\\nComparación de Modelos:\")\n",
        "print(f\"Forward Selection - AIC: {aic_forward}, BIC: {bic_forward}\")\n",
        "print(f\"Backward Elimination - AIC: {aic_backward}, BIC: {bic_backward}\")\n",
        "print(f\"Stepwise Selection - AIC: {aic_stepwise}, BIC: {bic_stepwise}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE, SequentialFeatureSelector\n",
        "from statsmodels.tools import add_constant\n",
        "import statsmodels.api as sm\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Cargar el conjunto de datos Ames Housing desde OpenML\n",
        "data = fetch_openml(name=\"house_prices\", as_frame=True)\n",
        "df = data.frame\n",
        "\n",
        "# Seleccionar características relevantes y variable objetivo\n",
        "X = df.select_dtypes(include=[np.number]).drop(columns=\"SalePrice\").fillna(0)\n",
        "y = df[\"SalePrice\"]\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Función para calcular AIC y BIC\n",
        "def calculate_aic_bic(model, X, y):\n",
        "    X_sm = add_constant(X)  # Agregar constante para el intercepto\n",
        "    model_sm = sm.OLS(y, X_sm).fit()\n",
        "    return model_sm.aic, model_sm.bic\n",
        "\n",
        "# 1. Método de Selección Hacia Adelante (Forward Selection)\n",
        "forward_selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction='forward')\n",
        "forward_selector.fit(X_train, y_train)\n",
        "selected_forward = X_train.columns[forward_selector.get_support()]\n",
        "print(\"Variables seleccionadas por Forward Selection:\", selected_forward)\n",
        "\n",
        "# Calcular AIC y BIC para el modelo Forward\n",
        "aic_forward, bic_forward = calculate_aic_bic(LinearRegression(), X_train[selected_forward], y_train)\n",
        "print(\"AIC Forward:\", aic_forward)\n",
        "print(\"BIC Forward:\", bic_forward)\n",
        "\n",
        "# 2. Método de Eliminación Hacia Atrás (Backward Elimination)\n",
        "backward_selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select=5, direction='backward')\n",
        "backward_selector.fit(X_train, y_train)\n",
        "selected_backward = X_train.columns[backward_selector.get_support()]\n",
        "print(\"Variables seleccionadas por Backward Elimination:\", selected_backward)\n",
        "\n",
        "# Calcular AIC y BIC para el modelo Backward\n",
        "aic_backward, bic_backward = calculate_aic_bic(LinearRegression(), X_train[selected_backward], y_train)\n",
        "print(\"AIC Backward:\", aic_backward)\n",
        "print(\"BIC Backward:\", bic_backward)\n",
        "\n",
        "# 3. Método de Selección Paso a Paso (Stepwise Selection)\n",
        "def stepwise_selection(X, y, initial_list=[], threshold_in=0.01, threshold_out=0.05):\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "        excluded = list(set(X.columns) - set(included))\n",
        "        new_pval = pd.Series(index=excluded)\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(X[included + [new_column]])).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        best_pval = new_pval.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_feature = new_pval.idxmin()\n",
        "            included.append(best_feature)\n",
        "            changed = True\n",
        "\n",
        "        model = sm.OLS(y, sm.add_constant(X[included])).fit()\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max()\n",
        "        if worst_pval > threshold_out:\n",
        "            changed = True\n",
        "            worst_feature = pvalues.idxmax()\n",
        "            included.remove(worst_feature)\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return included\n",
        "\n",
        "# Ejecutar Stepwise selection\n",
        "selected_stepwise = stepwise_selection(X_train, y_train)\n",
        "print(\"Variables seleccionadas por Stepwise Selection:\", selected_stepwise)\n",
        "\n",
        "# Calcular AIC y BIC para el modelo Stepwise\n",
        "aic_stepwise, bic_stepwise = calculate_aic_bic(LinearRegression(), X_train[selected_stepwise], y_train)\n",
        "print(\"AIC Stepwise:\", aic_stepwise)\n",
        "print(\"BIC Stepwise:\", bic_stepwise)\n",
        "\n",
        "# Comparación de Modelos\n",
        "print(\"\\nComparación de Modelos:\")\n",
        "print(f\"Forward Selection - AIC: {aic_forward}, BIC: {bic_forward}\")\n",
        "print(f\"Backward Elimination - AIC: {aic_backward}, BIC: {bic_backward}\")\n",
        "print(f\"Stepwise Selection - AIC: {aic_stepwise}, BIC: {bic_stepwise}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCbKMGitiogz",
        "outputId": "3d200196-e167-4336-9989-b7c12fa98ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables seleccionadas por Forward Selection: Index(['OverallQual', 'BsmtFinSF1', 'TotalBsmtSF', 'GrLivArea', 'GarageArea'], dtype='object')\n",
            "AIC Forward: 27738.565058197728\n",
            "BIC Forward: 27768.943347178058\n",
            "Variables seleccionadas por Backward Elimination: Index(['OverallQual', 'YearBuilt', 'BsmtUnfSF', 'TotalBsmtSF', 'GrLivArea'], dtype='object')\n",
            "AIC Backward: 27748.349242911743\n",
            "BIC Backward: 27778.727531892073\n",
            "Variables seleccionadas por Stepwise Selection: ['OverallQual', 'GrLivArea', 'BsmtFinSF1', 'GarageArea', 'TotalBsmtSF', 'YearRemodAdd', 'MSSubClass', 'MasVnrArea', 'LotArea', 'BedroomAbvGr', 'YearBuilt', 'OverallCond', 'GarageYrBlt', 'TotRmsAbvGrd', 'KitchenAbvGr', 'Fireplaces', 'LotFrontage']\n",
            "AIC Stepwise: 27476.489651575368\n",
            "BIC Stepwise: 27567.624518516353\n",
            "\n",
            "Comparación de Modelos:\n",
            "Forward Selection - AIC: 27738.565058197728, BIC: 27768.943347178058\n",
            "Backward Elimination - AIC: 27748.349242911743, BIC: 27778.727531892073\n",
            "Stepwise Selection - AIC: 27476.489651575368, BIC: 27567.624518516353\n"
          ]
        }
      ]
    }
  ]
}